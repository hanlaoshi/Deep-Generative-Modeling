# Deep-Generative-Modeling

This is a repository about Deep Generative Modeling(More attention to  probabilistic time series forecasting with Normalizing Flows) 

Updating everyday

Contact me: hanaif@mail2.sysu.edu.cn</font> 


# <table><tr><td bgcolor=orange> ðŸ”¥Book for generative modeling</td></tr></table>

## 2022
- Deep Generative Modeling (Contained code)   [[Book]](https://link.springer.com/book/10.1007/978-3-030-93158-2)


# <table><tr><td bgcolor=orange> ðŸ”¥Paper for generative modeling</td></tr></table>

## 2022
- E(n) Equivariant Normalizing Flows. [[Paper]](https://arxiv.org/abs/2105.09016) [[Code]](https://github.com/vgsatorras/en_flows)

        Introduces equivariant graph neural networks into the normalizing flow framework which combine to give invertible equivariant functions. Demonstrates their flow beats prior equivariant models and allows sampling of molecular configurations with positions, atom types and charges.

- BayesFlow: Learning complex stochastic models with invertible neural networks.  [[Paper]](https://arxiv.org/abs/2003.06281) [[Code]](https://github.com/stefanradev93/BayesFlow)
- PFVAE: A Planar Flow-Based Variational Auto-Encoder Prediction Model for Time Series Data.  [[Paper]](https://www.mdpi.com/2227-7390/10/4/610)

## 2021
- CInC Flow: Characterizable Invertible 3x3 Convolution. [[Paper]](https://arxiv.org/abs/2107.01358)  [[Code]](https://github.com/Naagar/Normalizing_Flow_3x3_inv)

        These authors sought to improve the emerging convolutions as they were expensive. So they investigated the conditions for when 3x3 convolutions are invertible under which conditions (e.g. padding) and saw successful speedups. Furthermore, they developed a more expressive, invertible Quad coupling layer. 

- Orthogonalizing Convolutional Layers with the Cayley Transform.  [[Paper]](https://arxiv.org/abs/2104.07167)  [[Code]](https://github.com/locuslab/orthogonal-convolutions)

        The authors parameterized the multichannel convolution to be orthogonal via the Cayley transform (skew-symmetric convolutions in the Fourier domain). This enables the inverse to be computed efficiently . 

- Automatic variational inference with cascading flows.  [[Paper]](http://proceedings.mlr.press/v139/ambrogioni21a.html) 

        This paper combine the flexibility of normalizing flows and the prior-embedding property of ASVI in a new family of variational programs, which named cascading flows.

- Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models.   [[Paper]](https://arxiv.org/abs/2103.04922) [[Datasets Used]](https://paperswithcode.com/dataset/cifar-10)
- Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows.  [[Paper]](https://arxiv.org/abs/2002.06103)   [[Code]](https://github.com/zalandoresearch/pytorch-ts)
- Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. [[Paper]](http://proceedings.mlr.press/v139/rasul21a.html)  [[Code]](https://github.com/zalandoresearch/pytorch-ts)

## 2020
- Invertible DenseNets.  [[Paper]](https://arxiv.org/abs/2010.02125#)
- VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation.  [[Paper]](https://arxiv.org/abs/1903.01434) [[Code]](https://github.com/tensorflow/tensor2tensor)
- Normalizing Kalman Filters for Multivariate Time Series Analysis. [[Paper]](https://proceedings.neurips.cc/paper/2020/hash/1f47cef5e38c952f94c5d61726027439-Abstract.html) 

## 2019
- Block Neural Autoregressive Flow.  [[Paper]](https://arxiv.org/abs/1904.04676)
- Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design. [[Paper]](https://proceedings.mlr.press/v97/ho19a.html) [[Code]](https://github.com/aravind0706/flowpp)  [[Dataset Used 1]](https://paperswithcode.com/dataset/cifar-10)  [[Dataset Used 2]](https://paperswithcode.com/dataset/imagenet)
- Sum-of-Squares Polynomial Flow. [[Paper]](http://proceedings.mlr.press/v97/jaini19a.html)

## 2018
- Neural Processes. [[Paper]](https://arxiv.org/abs/1807.01622)   [[Code]](https://github.com/deepmind/neural-processes)
        
         This paper introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like Gaussian process, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. 
        
- Sylvester Normalizing Flows for Variational Inference. [[Paper]](https://arxiv.org/abs/1803.05649) [[Code]](https://github.com/riannevdberg/sylvester-flows)
- Conditional Recurrent Flow: Conditional Generation of Longitudinal Samples with Applications to Neuroimaging. [[Paper]](https://arxiv.org/abs/1811.09897) [[Dataset Used]](https://paperswithcode.com/dataset/moving-mnist)

## 2017
- Masked Autoregressive Flow for Density Estimation. [[Paper]](https://arxiv.org/abs/1705.07057)  [[Code]](https://github.com/gpapamak/maf) [[Dataset Used 1]](https://paperswithcode.com/dataset/cifar-10)  [[Dataset Used 2]](https://paperswithcode.com/dataset/mnist) [[Dataset Used 3]](https://paperswithcode.com/dataset/bsd) [[Dataset Used 4]](https://paperswithcode.com/dataset/uci-machine-learning-repository) 
 

